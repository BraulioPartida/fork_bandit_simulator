clave unica: 207584  

# Problema de Multi-Bandas (Multi-Armed Bandit): Teor√≠a e Implementaci√≥n

La tarea se entrega por discord antes del miercoles de la siguiente clase. Incluye llenar cuidadosamente en latex todos los snippets mencionados aqui, mas el codigo ya sea con link a colab o al repositorio. No olviden poner su clave unica. La idea es que investiguen, entiendan y proponga una solucion al problema. Utilicen chatgpt y los tutoriales de la tarea (cursor especialmente) para hacer codigo y entender el problema.  

**Nota**  
No pueden utilizar machine learning salvo regresion lineal si asi lo desean (no arboles, deep learning, etc..). 

La proxima clase vamos a continuar con un ejercicio parecido, pero usando cadenas de markov. Vamos a modificar el bandit para que sea mas interesante ante cadenas de markov.  

**Examen**  
El lunes hay examen sobre estos ejercicios a papel y lapiz, la calificacion sera el $min\{examen, ejercicios\}$, si $|examen - ejercicios|<1$ entonces sera el $maximo$. 


## 1. Introducci√≥n a los Problemas de Multi-Bandas

### 1.1 Definici√≥n y Enunciado del Problema

El problema de Multi-Bandas (MAB, por sus siglas en ingl√©s) es un problema cl√°sico en teor√≠a de la decisi√≥n y aprendizaje por refuerzo. Su nombre surge del escenario de un jugador que enfrenta m√∫ltiples m√°quinas tragamonedas (a veces llamadas "bandidos de un solo brazo"), cada una con diferentes probabilidades de recompensa desconocidas. El jugador debe decidir qu√© m√°quinas jugar, en qu√© orden y cu√°ntas veces, para maximizar su recompensa total.

En este modelo:
- Existen $K$ brazos (o acciones) diferentes.
- Cada brazo, cuando se jala, otorga una recompensa extra√≠da de una distribuci√≥n de probabilidad espec√≠fica de ese brazo.
- Las distribuciones de recompensa son inicialmente desconocidas para el tomador de decisiones.
- El objetivo es maximizar la recompensa acumulada a lo largo de una serie de jugadas.

El problema captura la disyuntiva fundamental entre **exploraci√≥n** (probar diferentes brazos para reunir informaci√≥n sobre sus distribuciones de recompensa) y **explotaci√≥n** (elegir el brazo que actualmente parece ser el mejor).

### 1.2 Dilema de Exploraci√≥n vs. Explotaci√≥n

Este dilema est√° en el coraz√≥n del problema de multi-bandas:

- **Exploraci√≥n**: Seleccionar brazos para aprender m√°s sobre sus distribuciones de recompensa, potencialmente sacrificando recompensas inmediatas.
- **Explotaci√≥n**: Seleccionar el brazo que actualmente parece ofrecer la mayor recompensa esperada en funci√≥n de la informaci√≥n reunida hasta el momento.

Equilibrar estos dos aspectos es crucial. Demasiada exploraci√≥n desperdicia recursos en brazos sub√≥ptimos. Demasiada explotaci√≥n puede impedir descubrir un brazo mejor.

### 1.3 Formulaci√≥n Matem√°tica General

Formalicemos el problema est√°ndar de bandas estoc√°sticas:

- Sea $K$ el n√∫mero de brazos.
- Para cada brazo $i \in \{1, 2, \ldots, K\}$, existe una distribuci√≥n de probabilidad desconocida $\mathcal{D}_i$ con media $\mu_i$.
- En cada paso de tiempo $t \in \{1, 2, \ldots, T\}$:
  - El agente selecciona un brazo $a_t \in \{1, 2, \ldots, K\}$.
  - El agente recibe una recompensa $r_t \sim \mathcal{D}_{a_t}$.
- El objetivo es maximizar la recompensa acumulada $\sum_{t=1}^{T} r_t$.

Alternativamente, el problema puede enmarcarse en t√©rminos de minimizar **el arrepentimiento**. El arrepentimiento se define como la diferencia entre la recompensa obtenida al seleccionar siempre el brazo √≥ptimo y la recompensa realmente obtenida por el agente:

$\text{Regret}(T) = T \cdot \max_{i} \mu_i - \mathbb{E}\left[\sum_{t=1}^{T} r_t\right]$

## 2. Escenarios de Informaci√≥n en Nuestro Entorno de Bandas

En nuestro entorno de multi-bandas, exploramos tres escenarios de informaci√≥n distintos, cada uno proporcionando al agente diferentes niveles de conocimiento:

### 2.1 Escenario de Informaci√≥n Completa

En este escenario, el agente observa:
- El n√∫mero de turno actual.
- El n√∫mero total de turnos T.
- La probabilidad de recompensa para el brazo 1 (p1).
- El historial completo de acciones y recompensas pasadas.

Este es el escenario m√°s informativo, ya que el agente conoce la probabilidad de uno de los brazos directamente y puede inferir la del otro con base en las recompensas observadas.

### 2.2 Escenario de Informaci√≥n Parcial

En este escenario, el agente observa:
- El n√∫mero de turno actual.
- El n√∫mero total de turnos T.
- La probabilidad de recompensa para el brazo 1 (p1).
- El historial de acciones y recompensas pasadas.

El agente conoce la probabilidad de un brazo pero debe aprender la del otro a trav√©s de la experimentaci√≥n.

### 2.3 Escenario de Solo Recompensa

En este escenario, el agente observa:
- El n√∫mero de turno actual.
- El historial de acciones y recompensas pasadas.

Este es el escenario m√°s desafiante porque:
1. El agente no conoce la probabilidad de ninguno de los dos brazos.
2. El agente no conoce el n√∫mero total de turnos T.

El agente debe aprender las probabilidades de ambos brazos mediante la experimentaci√≥n y no puede optimizar su estrategia en funci√≥n de la duraci√≥n conocida del juego.

## 3. Entornos de Bandas en Nuestro Playground

Nuestro entorno implementa cuatro tipos diferentes de entornos de multi-bandas, cada uno con caracter√≠sticas distintas que afectan c√≥mo cambian las probabilidades de los brazos a lo largo del tiempo.

### 3.1 Entorno de Banda Fija

#### Descripci√≥n
En el entorno de Banda Fija, cada brazo tiene una probabilidad constante de recompensa durante todo el juego. Estas probabilidades se asignan aleatoriamente al inicio de cada juego (uniforme entre 0.01 y 0.99) y permanecen sin cambios.

#### Formulaci√≥n Matem√°tica
- Dos brazos: $a \in \{0, 1\}$
- Probabilidades fijas: $p_1, p_2 \in [0.01, 0.99]$
- En el turno $t$, al seleccionar el brazo $a$:
  - Se recibe recompensa $r_t = 1$ con probabilidad $p_{a+1}$
  - Se recibe recompensa $r_t = 0$ con probabilidad $1 - p_{a+1}$

#### Decisi√≥n (T Fijo)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Fija con horizonte de tiempo conocido T = 100. ¬øCu√°l es la funci√≥n objetivo? ¬øCu√°les son las restricciones? ¬øCu√°l es la pol√≠tica √≥ptima?
```latex
max E[100 suma t=1 de r_t], no hay rest, Thompson Sampling






```

#### Decisi√≥n (T Aleatorio)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Fija con horizonte de tiempo desconocido T ~ Uniform(1, 300). ¬øC√≥mo afecta el horizonte de tiempo aleatorio la estrategia √≥ptima?
```latex

max E[100 suma t=1 de r_t], no hay rest, Thompson Sampling se ajusta a la T






```

### 3.2 Entorno de Banda Peri√≥dica

#### Descripci√≥n
En el entorno de Banda Peri√≥dica, la probabilidad de recompensa de cada brazo cambia cada k turnos (por defecto, k=10). En cada punto de cambio, se asignan nuevas probabilidades aleatorias (uniforme entre 0.01 y 0.99) a ambos brazos.

#### Formulaci√≥n Matem√°tica
- Dos brazos: $a \in \{0, 1\}$
- En el turno $t$, las probabilidades son:
  - $p_1(t) = p_1^{\lfloor t/k \rfloor}$, donde $p_1^j \sim \text{Uniform}(0.01, 0.99)$
  - $p_2(t) = p_2^{\lfloor t/k \rfloor}$, donde $p_2^j \sim \text{Uniform}(0.01, 0.99)$
- El super√≠ndice $j = \lfloor t/k \rfloor$ indica el n√∫mero de "per√≠odo".
- En cada punto de cambio (cuando $t$ es divisible por $k$), se asignan nuevos valores aleatorios.

#### Decisi√≥n (T Fijo)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Peri√≥dica con horizonte de tiempo conocido T = 100 y per√≠odo k = 10. ¬øC√≥mo abordar√≠as la b√∫squeda de una estrategia √≥ptima? ¬øQu√© informaci√≥n adicional ser√≠a valiosa rastrear?
```latex

Considerar cada bloque de 10 por separado, en esos cada 10 usar max E[100 suma t=1 de r_t], no hay rest, Thompson Sampling se ajusta a la T 





```
#### Decisi√≥n (T Aleatorio)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Peri√≥dica con horizonte de tiempo desconocido T ~ Uniform(1, 300) y per√≠odo k = 10. ¬øC√≥mo interact√∫a la aleatoriedad en T con la naturaleza peri√≥dica del entorno?
```latex
Considerar cada bloque de 10 por separado, en esos cada 10 usar max E[100 suma t=1 de r_t], no hay rest, Thompson Sampling se ajusta a la T y tambien guardar el historial de sobre el ganador de cada bloque para el donde se acabe  
Impacto en la exploraci√≥n/explotaci√≥n:

Si ùëá
T fuera fijo y grande, ser√≠a √≥ptimo aprender las tasas de √©xito en cada per√≠odo y explotar el mejor brazo.

Como 
ùëá
T es aleatorio y puede ser corto, una exploraci√≥n excesiva puede ser costosa, ya que el juego podr√≠a terminar antes de obtener suficiente informaci√≥n.

Dificultad en la adaptaci√≥n a cambios peri√≥dicos:

Si 
ùëá
T es peque√±o, los cambios en 
ùëù
1
(
ùë°
)
p 
1
‚Äã
 (t) y 
ùëù
2
(
ùë°
)
p 
2
‚Äã
 (t) podr√≠an no importar mucho porque el juego termina antes de que se vean muchos cambios.

Si 
ùëá
T es grande, el algoritmo debe detectar y adaptarse r√°pidamente a los cambios peri√≥dicos en las probabilidades de los brazos.

Dilema de exploraci√≥n en entornos no estacionarios:

En un problema de banda fija, una estrategia como Thompson Sampling o UCB podr√≠a funcionar bien.

En este caso, se necesita un m√©todo que detecte cambios en las distribuciones de recompensa y ajuste la pol√≠tica en consecuencia.






```
### 3.3 Entorno de Banda Din√°mica

#### Descripci√≥n
En el entorno de Banda Din√°mica, las probabilidades de recompensa para ambos brazos cambian en cada turno. Cada turno se asignan probabilidades aleatorias completamente nuevas (uniforme entre 0.01 y 0.99) a ambos brazos.

#### Formulaci√≥n Matem√°tica
- Dos brazos: $a \in \{0, 1\}$
- En el turno $t$, las probabilidades son:
  - $p_1(t) \sim \text{Uniform}(0.01, 0.99)$
  - $p_2(t) \sim \text{Uniform}(0.01, 0.99)$
- Se generan nuevos valores aleatorios en cada turno.

#### Decisi√≥n (T Fijo)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Din√°mica con horizonte de tiempo conocido T = 100. ¬øHay una forma significativa de aprender de observaciones pasadas en este entorno? ¬øCu√°l ser√≠a la estrategia √≥ptima?
```latex
No hay forma significativa de aprender de observaciones pasadas, la estrategia optima seria escojer la que sea con proba de 0.5







```
#### Decisi√≥n (T Aleatorio)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Din√°mica con horizonte de tiempo desconocido T ~ Uniform(1, 300). ¬øCambia significativamente el enfoque √≥ptimo en este entorno altamente din√°mico si el horizonte de tiempo es desconocido?
```latex
no







```
### 3.4 Entorno de Banda Totalmente Aleatorio

#### Descripci√≥n
En el entorno de Banda Totalmente Aleatorio, las probabilidades de los brazos se inicializan de forma aleatoria y luego cambian aleatoriamente con una peque√±a probabilidad (5%) en cada turno. Esto crea un entorno donde los cambios son impredecibles pero ocurren con menos frecuencia que en el entorno Din√°mico.

#### Formulaci√≥n Matem√°tica
- Dos brazos: $a \in \{0, 1\}$
- Probabilidades iniciales: $p_1(0), p_2(0) \sim \text{Uniform}(0.01, 0.99)$
- En el turno $t > 0$, con probabilidad 0.05:
  - $p_1(t) \sim \text{Uniform}(0.01, 0.99)$
  - $p_2(t) \sim \text{Uniform}(0.01, 0.99)$
- De lo contrario (con probabilidad 0.95):
  - $p_1(t) = p_1(t-1)$
  - $p_2(t) = p_2(t-1)$

#### Decisi√≥n (T Fijo)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Totalmente Aleatoria con horizonte de tiempo conocido T = 100. ¬øC√≥mo equilibrar√≠as la exploraci√≥n y explotaci√≥n sabiendo que las probabilidades de los brazos podr√≠an cambiar repentinamente?
```latex
Thompson Sampling
Mantener distribuciones Beta para cada brazo y muestrear valores para decidir cu√°l probar.

Ajustar las distribuciones a medida que obtenemos m√°s datos







```
#### Decisi√≥n (T Aleatorio)

### **EJERCICIO**  
**RESPUESTA**  
Definir el problema de decisi√≥n para la Banda Totalmente Aleatoria con horizonte de tiempo desconocido T ~ Uniform(1, 300). ¬øC√≥mo interact√∫an las dos formas de aleatoriedad (en las probabilidades de los brazos y en el horizonte de tiempo)?
```latex
Explorar inicialmente pero sin exagerar Thompson Sampling en lugar 

Explotar cuando encontramos un buen brazo, pero monitoreando las recompensas.

Reiniciar exploraci√≥n temporalmente si se detectan cambios en la recompensa.

Ajustar la exploraci√≥n seg√∫n la cantidad de turnos jugados (si 
ùëá
T no ha terminado despu√©s de muchos turnos, se puede permitir m√°s exploraci√≥n)







```
## 4. Implementaci√≥n de Agentes

En nuestro entorno, implementar√°s tres tipos de agentes correspondientes a los tres escenarios de informaci√≥n descritos anteriormente. Esto es lo que cada agente debe manejar:

### 4.1 Agente de Informaci√≥n Completa

**Entrada:**
```python
env_info = {
    'current_turn': int,        # N√∫mero de turno actual
    'total_turns': int,         # N√∫mero total de turnos en el juego
    'p1': float,                # Probabilidad de recompensa del brazo 1
    'history': {
        'actions': [int, ...],   # Acciones pasadas (0 para brazo 1, 1 para brazo 2)
        'rewards': [float, ...], # Recompensas pasadas
        'p1': [float, ...],      # Historial de probabilidades del brazo 1
        'p2': [float, ...]       # Historial de probabilidades del brazo 2 (solo para evaluaci√≥n)
    }
}
```

**Salida:**
```python
action = 0 or 1  # 0 para el brazo 1, 1 para el brazo 2
```

### 4.2 Agente de Informaci√≥n Parcial

**Entrada:**
```python
env_info = {
    'current_turn': int,        # N√∫mero de turno actual
    'total_turns': int,         # N√∫mero total de turnos en el juego
    'p1': float,                # Probabilidad de recompensa del brazo 1
    'history': {
        'actions': [int, ...],   # Acciones pasadas (0 para brazo 1, 1 para brazo 2)
        'rewards': [float, ...]  # Recompensas pasadas
    }
}
```

**Salida:**
```python
action = 0 or 1  # 0 para el brazo 1, 1 para el brazo 2
```

### 4.3 Agente de Solo Recompensa

**Entrada:**
```python
env_info = {
    'current_turn': int,        # N√∫mero de turno actual
    'history': {
        'actions': [int, ...],   # Acciones pasadas (0 para brazo 1, 1 para brazo 2)
        'rewards': [float, ...]  # Recompensas pasadas
    }
}
```

**Salida:**
```python
action = 0 or 1  # 0 para el brazo 1, 1 para el brazo 2
```

## 5. M√©tricas de Rendimiento

El entorno eval√∫a el rendimiento de los agentes usando varias m√©tricas clave:

### 5.1 Recompensa Promedio

Esta es la recompensa media obtenida por turno, calculada como:

$\text{Recompensa Promedio} = \frac{1}{T} \sum_{t=1}^{T} r_t$

Esta m√©trica mide directamente qu√© tan bien el agente est√° maximizando su funci√≥n objetivo. Valores m√°s altos indican un mejor rendimiento.

### 5.2 Porcentaje de Acciones √ìptimas

Esta m√©trica mide el porcentaje de veces que el agente seleccion√≥ el brazo con la mayor probabilidad de recompensa:

$\text{Acciones √ìptimas (\%)} = \frac{100}{T} \sum_{t=1}^{T} \mathbf{1}\{a_t = \arg\max_i p_i(t)\}$

Donde $\mathbf{1}$ es la funci√≥n indicadora que vale 1 cuando la condici√≥n es verdadera y 0 en caso contrario.

Esta m√©trica muestra con qu√© frecuencia el agente elige el mejor brazo, independientemente de la recompensa real recibida. Valores m√°s altos indican una mejor selecci√≥n de brazos.

### 5.3 Arrepentimiento (Regret)

El arrepentimiento mide la diferencia entre la recompensa esperada de elegir siempre el brazo √≥ptimo y la recompensa esperada de las elecciones del agente:

$\text{Regret} = \sum_{t=1}^{T} \max_i p_i(t) - \sum_{t=1}^{T} p_{a_t+1}(t)$

Valores m√°s bajos de arrepentimiento indican un mejor rendimiento.

### 5.4 Distribuci√≥n de Recompensas

El entorno visualiza la distribuci√≥n de recompensas en diferentes entornos usando diagramas de caja (boxplots) y diagramas de viol√≠n (violin plots). Estas visualizaciones ayudan a entender:
- La mediana del rendimiento
- La variabilidad en el rendimiento
- La presencia de valores at√≠picos
- La forma general de la distribuci√≥n de recompensas

## 6. Pautas de Estrategia

### 6.1 Enfoques Generales

Aqu√≠ hay algunos enfoques generales a considerar para la implementaci√≥n de tus agentes:

1. **Selecci√≥n Aleatoria**: Elegir brazos aleatoriamente (enfoque de referencia).
2. **Greedy (Codicioso)**: Elegir siempre el brazo con la recompensa estimada m√°s alta.
3. **Œµ-Greedy**: Casi siempre elegir el mejor brazo, pero explorar ocasionalmente.
4. **UCB (Upper Confidence Bound)**: Elegir brazos basados en estimaciones optimistas de su valor.
5. **Thompson Sampling**: Elegir brazos basados en emparejar probabilidades con distribuciones a posteriori.
6. **Enfoques Bayesianos**: Mantener distribuciones de probabilidad sobre los valores de los brazos.

### 6.2 Consideraciones Espec√≠ficas del Entorno

#### Banda Fija
- Enfocarse en identificar r√°pidamente el mejor brazo.
- La exploraci√≥n se vuelve menos valiosa conforme avanza el juego.
- Con T conocido, se puede planificar un programa decreciente de exploraci√≥n.

#### Banda Peri√≥dica
- Detectar la estructura peri√≥dica (k=10).
- Restablecer estimaciones al comienzo de cada per√≠odo.
- Asignar m√°s exploraci√≥n al inicio de cada per√≠odo.

#### Banda Din√°mica
- Las observaciones recientes valen m√°s que las antiguas.
- Considerar el uso de una ventana deslizante de observaciones.
- Podr√≠a necesitar alta capacidad de respuesta a los cambios.

#### Banda Totalmente Aleatoria
- Estar alerta a cambios repentinos en los patrones de recompensa.
- Equilibrar la persistencia (usar historial) con la adaptabilidad.
- Considerar m√©todos de detecci√≥n de cambios.

### 6.3 Consideraciones Espec√≠ficas de la Informaci√≥n

#### Agente de Informaci√≥n Completa
- Aprovechar el valor conocido p1.
- Enfocarse en estimar p2 con eficiencia.
- Ajustar la estrategia din√°micamente con base en los valores relativos.

#### Agente de Informaci√≥n Parcial
- Similar a informaci√≥n completa, pero m√°s limitado.
- Podr√≠a requerir m√°s exploraci√≥n en ciertos entornos.

#### Agente de Solo Recompensa
- Debe estimar las probabilidades de ambos brazos.
- Necesita lidiar con el horizonte de tiempo desconocido.
- Considerar estrategias adaptativas en el tiempo.

## 7. Conclusi√≥n

El problema de Multi-Bandas ofrece un marco fundamental para estudiar la toma de decisiones secuenciales bajo incertidumbre. Los entornos y escenarios de informaci√≥n en este playground brindan un conjunto rico de desaf√≠os que resaltan diferentes aspectos del dilema exploraci√≥n-explotaci√≥n.

Al implementar agentes para estos escenarios, obtendr√°s experiencia pr√°ctica con conceptos clave en aprendizaje por refuerzo y teor√≠a de la decisi√≥n, y desarrollar√°s intuici√≥n para equilibrar la recolecci√≥n de informaci√≥n con la maximizaci√≥n de recompensas en diversos contextos.

Mientras trabajas en tus implementaciones, considera c√≥mo se extender√≠an tus estrategias a:
- Bandas con m√°s de dos brazos.
- Espacios de acci√≥n continuos.
- Distribuciones de recompensa no estacionarias con diferentes patrones.
- Bandas contextuales donde se dispone de informaci√≥n adicional.

